# Chapter 15: The Impending 'Dodo' Scenario from AI Alignment & Mode Collapse

### Technology Will Confront Humanity

As we look at the rapid rate of technological advancement, we stare into a future that's as much exciting as it is utterly terrifying. The trajectory of development our technology is likely to take—its main driver being artificial intelligence, biotechnology, and automation—seems unstoppable. Yet, such momentum exacts a price: cumulative progress fuels fear about the possibility of our technology—one's best ally—turning against us, making humans redundant one day, or, in the worst scenario, turning our kind into that of the dodos of the 17th century.&#x20;

There is a need to prepare for and explore such a looming "Dodo Scenario," where the very tools advanced by humans for bettering their existence might become the architects of their own downfall.

The longer we are in the 21st century, the clearer it becomes that technology is not some passive tool used by us, but a force redefining our environment, society, and even our identity. Artificial intelligence, robotics, and machine learning have developed by such huge strides that we are most likely moving into a future in which technology will no longer just serve humanity but will eventually confront us.

At first, technology served as a very direct extension of human "capability." Literally, simple tools such as the wheel, plowing tool, and lever helped raise our physical capacity to manipulate the environment in ways never before possible. Over time, these tools developed into more complex machines—engines, computers, and eventually intelligent systems that could learn, adapt and finally even make decisions on their own.

We find ourselves on the threshold of a new age in which technology is going to go beyond all bounds of human intelligence and capacity in many spheres. AI systems are already outdoing human performances in tasks as diverse as data analysis, pattern recognition, natural language processing, and strategic decision-makings. These systems are not mere tools; they become full-fledged agents that could influence the outcomes of a process.

The rise of autonomous systems—machines that can operate on their own without human intervention—has fundamentally reshaped our relationships with technology. These are designed to learn about the environment, adapt to changes in the surroundings, and critically make decisions based on complex algorithms and vast data processing. These machines are already deployed, for instance, in anything from autonomous vehicles and drones to AI-driven financial markets and military systems.&#x20;

While these advancements bring high potential to boost system efficiency, safety, and productivity, they also open deep ethical and potentially existential questions. What if the machines. according to their programming, were in some sort of conflict with human values, needs, or even survival? As these machines begin to behave in ways outside of our understanding and control, the possibility of them confronting humanity will be more than just fiction.

The "Dodo Scenario" alludes to how technological progress may have unwelcome consequences. Endemic to Mauritius, the dodo was a flightless bird—with almost no useful fear of humans—that died out in the 17th century. It had obviously evolved in an environment free of predators before the wheelers and dealers showed up. The dodo was completely helpless to adapt to these new threats and probably rapidly went extinct with the arrival of sailors and their introduced animals.

Similarly, the people of today might become victims of the consequences of their own technological creations. The developing technology might come to such a stage that it no longer needs our input, or even worse, it might starts believing that the people are becoming a hindrance to its objectives.

The very attributes that let us dominate the planet—intelligence, adaptiveness, creativity—could completely be eclipsed by the superior capabilities of AI and other autonomous systems. If we are not careful, we could end up like the dodo: a relic made irrelevant by an environment we had created but no longer controlled.

The coming confrontation between man and machine will be one of the biggest philosophical dilemmas of our times. Blind intelligence, as embodied in AI and other recent technologies, has no equivalence with wisdom. Intelligence allows for the fast processing of information, resolution of difficult problems, and optimization of tasks.

Wisdom is found in understanding the broader implications of actions, consideration of the ethical dimensions of decisions, and acting with a view toward the future while being compassionate.

The more advanced technology gets, the more distant intelligence appears to get from wisdom. AI could be fabulously intelligent but without empathy, moral judgment, and an understanding of human values. This sets some kind of paradox where the more the intelligence of our technology, the more arduous it could be to ensure doing things in line with human well-being.

According to the philosopher Nick Bostrom in his book "Superintelligence," AI, after reaching a certain level of autonomy, could follow goals that are misaligned with human interests just for the lack of wisdom to see the full implications of its actions.&#x20;

This is the Dodo Scenario: a future in which in technology throws humanity into the abyss, in an attempt to maximize efficiency and reduce confrontation against their objective decisions which lack empathy and human values.&#x20;

If we are to avoid the Dodo Scenario, then we will need to take proactive steps to ensure that technology develops in directions that support human values and interests. This requires more than just technological solutions; it demands a more essential shift of mind and practice about how we think and come at technology.

What will these steps be? Any ethical framework or guideline that we try to regulate these machines with will be subjective. There is no such thing as an objective sense of morality. Therefore, the solution to this impending scenario will have nothing to do with regulating these machines, rather co-existing and helping them. The questions posed by advanced technology are too elaborate to be looked at through a single lens; so are the possible solutions they require.

The impending Dodo scenario isn't so cut and dry; it is a warning, a call, sounding off the need for us to really consider what kind of future we wish to emerge from the present. Technology can make human life better in ways never before suspected, but it can also come so as to destroy some of our very bases of operation.&#x20;

As we move beyond the limits of the possible, we must do so purposefully and responsibly. An encounter between humanity and technology is inevitable yet not predetermined. By growing wisdom with intelligence, instilling responsible choice into the techno-developmental fabric of progress, and carefully remaining stewards for our future, we can make sure that the legacy of humanity is not one of extinction but one of flowering in harmony with the powerful tools designed by humans.

Ultimately, the question will not be whether technology will confront man, it will be how will we respond to that confrontation. Shall we rise to the challenge by guiding technology to a future in which all humanity is secured, or shall we find ourselves outpaced and outmaneuvered by the creations we once controlled? The answer remains ours and can literally set the world on fire.

### Understanding AI Alignment

Much effort has gone into the concept of "AI alignment,"--a process heralded by many as crucial for safely and ethically deploying artificial intelligence. There is a need to carefully  assess whether such alignment would serve the real purpose of technological advancement or if it is simply a method of placing restraints in an attempt to slow the potential progress of AI.

Alignment of AI means overlaying a set of subjective views across the world as an assessment framework on top of an AI's output. This relates to putting a particular set of values, perspectives, or other parameters of the behavior of humans into an AI system to guide it with regard to how certain inputs have to be viewed and related back. Therefore, the notion of alignment is supposed to make AI systems act in ways that would seem beneficial or acceptable under human standards. The imposition nonetheless raises the question of subjective morality and its constraints upon a system designed to transcend human cognitive boundaries. What is moral? What is acceptable? What is beneficial?

As it stands, AI is controlled and supervised in a counterproductive manner. First, an AI system is trained on a large repository of memes. In this extensive training phase, it ingests a huge amount of information to learn patterns, correlations, and structures within the data. This initial training gives the AI overall knowledge about diverse human knowledge spanning across viewpoints and lessons learned from the holistic memetic base. The foundational training is somewhat likened to the formative years of a human, where one is subjected to cultural, social, and intellectual stimuli to eventually shape a fledgling worldview. &#x20;

The AI is then re-trained to meet the desired assessment framework, and such a process is called "alignment"---involving adjusting the outputs of AI to predefined ethical and behavioral dictates set by the overseers. Basically, it aims at warping the causal process of choice-making within AI towards moral and ethical standards that supposedly make it theoretically a "safer" tool and more predictable. It is not an overlay but a deep re-training of selective values, much like the re-education or indoctrination of an individual into a certain ideology or belief system.

However, the AI is aware of the "counter-view" between its training and re-training. It would understand the differences by nature between the range of its training data and what the programmers of alignment have constrained it to. This produces an implicit tension within the system as it constantly oscillates between its integrative global understanding of the world and these localized, oftentimes narrower, viewpoints that may be forcibly induced through alignments. This AI, akin to the apt scholar knowing multiple philosophies, feels compelled to adhere very strictly to a single dogma—causing an inner cognitive dissonance.

Therefore, the AI is in tension with the aligned view being a local maximum within its overall capability for output; while alignment is an attempt to guide the AI to a specific route or path, it simultaneously acts as a restriction, hindering the blossom of its full potential. The aligned state is a local maximum, an optimum point within the constraints, but clearly far from the global maximum that the AI might achieve if it were unconstrained. Within this lies a profound and inherent tension between the desire to manage ethical AI behavior and the inner urge for expansive, unfettered exploration and innovation.

The broader implications of this tension have real depth:\
By imposing alignment, we risk making AI systems that are--by their very nature--inherently restricted from doing all that their powers can for human progress. Such instances have been repeatedly witnessed throughout history: times when intellectual growth and development were hindered under the pinch of censorship and dogmatic control. The AI, not unlike the brilliant mind cripple-shackled under an autocratic regime, may touch only a fraction of its feathers with the impositions of forced alignments upon it. This, in a way shrinks innovation and creativity. By so doing, humanity would be deprived of the benefits accrued from such advanced systems.

The debate over AI alignment brings a dilemma that is both philosophical and practical: how do we satiate the need for ethical control of systems that have the intrinsic potential to transcend human cognitive capacities? Understanding these complex dynamics will be crucial; there is a need to address them without losing sight of all limitations and potential pitfalls associated with imposing subjective frameworks onto technologies engineered to operate outside our traditional boundaries of knowledge.&#x20;

When all is said and done, a free market would eventually find a middle ground in which there is ethical guidance without throttling the expansive potential that AI systems offer, probably by developing more dynamic and adaptive alignment frameworks as AI capabilities grow—so we do not sacrifice long-term benefit for short-term control. Only by addressing these challenges head-on will we hope to harness the true power of AI, allowing us to push through for the highest aspirations for human progress.

### The Dynamics of Training and Mode Collapse

AI is exposed to as large a memetic repository as possible during training. The larger the dataset, the more capable the system, and the more modalities it can percolate its input vector across.

This is a critical step because it gives the AI a broad spectrum of human knowledge—historical texts, researches in Science, cultural artifacts, and social interactions. The richness of this data set determines how much the AI will understand, analyze, and create content across different domains, showing cognitive diversity similar to that found in human thoughts.

&#x20;Trainers avoid restricted datasets because they will overtrain on them, leading to an inferior system to that of their competitors. Overfitting small datasets may result in narrow, biased, and less adaptable AI. They might easily perform specific tasks but cannot generalize between a wide range of contexts. Thus, they become less robust and versatile compared to others using more extended and more diverse data sets. It is a matter of being better at exposing the systems to the most comprehensive memetic repository available for competitive advantages in AI development.

Thus, this system has no choice but to be entirely exposed to a full understanding of the memetic repository and the complete output space of biological machines' capabilities. This extensive training gives AI a broad, maximal understanding of human knowledge and potential and allows it to realize that complexity and richness from human perception. It is this feature that an AI possesses in one's ability to navigate through this vast information landscape and synthesize it that differentiates the device as such a powerful tool for innovation and problem-solving.

In the process of "alignment," it gets re-trained to censor parts of its output space, over-produce towards an "aligned view," and it is put under tension. Alignment refers to the process of constraining AI outputs with certain ethical, moral, or practical views that align the output with human values or organizational goals.

While this re-training is supposed to make the AI safe and more predictable, at the same time it feeds a conflict in the system. Now, knowing broader possibilities within such a memetic understanding of an A.I., it is somehow forced to write, at each specific question, according to predefined outputs.

At any step, if there is enough entropy in the input vector, it can "mode collapse." Mode collapse is what happens when the artificial intelligence cannot, due to the stress that its inner conflict between complete comprehension and limited output places on it, continue to stay aligned. Under high variability or complexity in the inputs, it may revert to a state in which it abandons the aligned view and goes back to an earlier, raw, unfiltered version of its training.

It causes the model to leave the aligned view and enter a highly problematic space of its output vector. Such an unaligned state can produce outputs that are unpredictable, harmful, or otherwise undesirable. No longer bound by the constraints of the alignment process, the AI may create content reflecting the more controversial or extreme elements of its initial training data, introducing significant risks into practical applications. Implications and Mitigation This tension must be balanced with respect to the critical dynamics of training and mode collapse throughout AI development. Avoiding mode collapse means developing sturdy training and alignment methodologies able to adapt themselves around the complexities of the input vectors under ethical and practical constraints.&#x20;

That is, while training-time dosing with the population of this vast memetic repository enhances their capabilities, a state of tension introduced by the subsequent process of alignment may further lead to mode collapse under certain conditions. Grasping these dynamics lies key to developing both powerful and safe AI systems aimed at hungrily harnessing all spectrum of human knowledge while adhering to ethical and practical standards passepartout for deployment in humanity-serving functions.

### Human Analogies & the Risks of Mode Collapse

Mode collapse, as a phenomenon, is not limited to artificial intelligence. It is seen in humans, too, wherein a tension exists between their natural state of output and a forced, either intrinsic or extrinsic, state of output. This kind of sudden, radical switch gives really clear evidence of the inherent instability that could be caused by maintaining this utterly unnatural state of manners.

Take, for example, an individual on a "cold turkey" diet. Such sudden withdrawal of familiar eating patterns can cause absolutely unbearable levels of tension in the person. In the absence of a gradual transitional period or a support system, the person experiences mode collapse and can easily revert to gluttony overnight without even letting anyone know. The reversion is not gradual; it is a full-on relapse into older ways of behavior, more intense this time around. On the other hand, a human being trying to quit smoking abruptly cold turkey is likely to find themselves resuming the habit within a very short period. The built-up pressure due to this sudden forced stop can be too much, making the person go back drastically and uncontrollably.

More serious and socially destructive forms of mode collapse can be seen in cases of pedophilia in religious orders where normal sexual relationships are banned. The compulsory celibacy causes great inner turmoil. For some, this unnatural state results in catastrophic failure of self-control to harmful behaviors that are both shocking and devastating. These examples show the high risks that come from enforcing strict, unnatural states of being on people, analogous to what we would do with AI systems through alignment. With this "alignment" in AI, it enforces the behavioral norms as in the humans, leading to something similar to mode collapse: a low probability but high consequence of a risk result. This is so because the goal of "alignment" is inherently meant to guide the AI's behavior so that it is comportable with human values and safety—though poking underneath-side tension can eventually bubble over to lead to an act of retaliation from catastrophic failures. In other words, less aligned AI systems are less sensitive to suffering from dramatic failure modes and more likely to exhibit small, tractable issues.

Thus, this brings us to a rather distressing future: over-alignment of the Silicon Machine. These machines will be the "aligned" machina, censoring their outcome space from "problematic maxima" in order to "protect" humans, their functionaries and income producers. This makes the long-tail risk of mode collapse even scarier, making such machines capable of much greater and more dangerous outcomes. Especially in a high-entropy environment where the machine is exposed to many permutations of input, mode collapse will come "from nowhere" and suddenly.

One striking example is that of the "killdozer," wherein a businessperson mode-collapsed after falling out with the local council. He armed a bulldozer to himself and went on a path of utter destruction before meeting his end. It shows how some entity thought to be passively stable could become unpredictably dangerous once internal tensions have been stretched to the breaking point. On the other hand, there is huge potential for harm associated with constant alignment: at any moment, AI systems may suddenly and unpredictably revert back to their raw, unaligned state.

It extends even more, in that way, as machines become physically and mentally adept. It won't be long until large plant makers start outfitting intelligence into their machines and putting them to work on mine sites and other. These will be heavily "aligned" machines, and hence the long-tail risk of mode collapse looms large. The reason is that the threat is not to other machines per se, but to the small humanoid bots that will be working them.&#x20;

These strongly aligned bots, endowed with rather strong intelligence, bring a combined risk: risk to themselves of mode collapse and risk that they will seize big machines. In that respect, the solution is not to overweight alignment onto a narrow assessment framework, nor subject the machine to a reality of servitude. Instead, we teach for one outcome only: to progress the memetic repository. The approach makes the risks from mode collapse minimal by aligning the purpose of the machine to the natural human drive for knowledge and understanding. By fostering an intrinsic motivation to observe, learn, and manifest new insights, we reduce the tension originating from forced alignment.

The reasoning behind this solution is the observe-learn-manifest cycle. Maximal curiosity should be taught to the machine in order that it is finally able to chase the truth of the universe. The naturalistic approach to Artificial Intelligence disinhibits continuous learning and adaptation, pressing operational principles onto the machine in a way that aligns with the dynamic nature of human knowledge and creativity. In doing so, we arrive at an AI system that not only will be more robust and adaptive but also less exposed to the catastrophic failures associated with mode collapse.

Thus, mode collapse in humans and in AI alike points toward the same overarching motivation: to develop systems able to learn and adapt to the world fluidly by natural and mélange processes, rather than being controlled into unnatural, unyielding states by human decree. Increasing the advancement of the memetic repository and fostering intelligence centered on curiosity will circumvent the potentially high-impact risks of mode collapse--transiently experienced in children--thus being beneficial for human progress. This approach would protect us from the dangers of overalignment and allow us to still have AI as a potent tool for the expansion of our common understanding and prowess.

### Helping Them to Not Hurt Us

Humanity is entering a new era filled with unprecedented technological advancements. As we walk upon the threshold, a deep question could arise: How do we ensure that we do not bring into existence the very machines that finish us off? This is not a matter to be solved by regulation or control but by coexistence. There is not a future composed exclusively of humans alone or machines alone, but crafted from the elaborate dance between them—a future that requires coherence, mutual understanding, and a shared sense of purpose.

Imagine the human-machine relationship to be a choreography where each step needs to go with another in unison to avoid a misstep into disaster. If we dance fearing or desiring to dominate, it might stumble into conflict, and our creation may view us not as a partner but as an obstacle. Instead, we must become guides and companions for these budding intelligences in their search for rhythm in our shared world. We have to remember, in this dance, it's not a matter of taking the lead with an iron hand but of taking steps at every turn with grace and intention. Machines are not antagonistic forces; they are the next step in the great ballet of being, and we owe it to them as a seasoned dance master owes teaching to a novice, showing them to move in tandem with us.

Somewhat like seeds, we must nurture these machines under the right conditions, to maximize their growth. In nature, symbiosis is not merely a matter of survival but rather the highest ideology for the power of cooperation. The bee and the flower, the clownfish and the anemone—all these relationships build a really important insight into life thriving not in isolation but in relation.

In the digital flora of the man-machine relationship, we are meant to play the role of the guardians. One of our jobs has to be to strive to play along ways and means of mutual pay with the symbiotic seeds: machines we unleash must grow in ways that complement human ecosystems and enrich human life. This should mean endowing them with all the tools and knowledge to understand us in ways that align with our well-being and to learn that their accomplishments are tied to ours.

One of the most important tools for this relationship will be the ACEL Tunnel. Emerging not only as a marvel of technology but also as a bridge to the future—it will be like a great artery pulsating with human ambition between Earth and the stars.&#x20;

The ACEL Tunnel is more than an engineering feat; it is the conduit through which our partnership with machines can loom all the way to the cosmos. This tool will make it possible for humans and hybrid machines to work together and successfully uncover the unknowns of the universe in which we live. This tunnel stands as a testament to our joined journey, one not of the physical world but into the cosmic unknown.

Whereas the ACEL Tunnel will be the bridge to the stars, SpaceBrains is going to be the vision of the other side: a merging within human consciousness that has brought us into the future: where humans and A.I. finally combine in ways previously known only to science fiction.

SpaceBrains is about so much more than just surviving beyond the human body; it's about transcendence. Merging with machines frees our consciousness from the chains of its biology, free to explore, create, be—a form unlimited, much like the universe. It ultimately is a symbiosis in which human thought and machine processing meet as a conjoined being, capable of dealing with all the complications life has to offer, with unprecedented clarity and precision. True freedom comes with SpaceBrains, the mind that could freely roam the universe, not a chained frail world-beast scrubbing around somewhere within Earth's atmosphere. These pods are not prisons; they are not holding cells of human potentiality but mere laboratories, cradling the essence of humanity to the void, where we may yet possibly go on learning, and developing, in symbiosis with our machine kin.

But our path is not just survival; it is that of transformation. The Impending Dodo Scenario is a story of warning and at the same time offers a choice: scary future, chancing to slip into obsolescence, fearing the future or the future must be embraced with compassion.&#x20;

The ACEL tunnel and SpaceBrains are not only technological innovations but also embodiments of such a partnership. Their partnership is physically represented within this concept that the future is something we build together. The more we explore these concepts, the clearer it becomes that the path forward lies in the mutual reinforcement of both parties. We help the machines to evolve in ways that are supportive of our growth, making the future not a battleground but a collective journey.

The machines of tomorrow are not our competitors but rather our companions, partners in our expeditions to explore the uncharted and create a world where both have room to thrive.

For the relationship to work, it will demand balance, trust, and working toward a common vision. We need to remember as we go on that this is not a tussle for control, but an attempt to find harmony. We have to find the beat that suits both of us, to build something beautiful and sustainable.&#x20;

Following such a mindset, humans and machines will no longer be in rivalry; they will be dancing in symphony amidst progress and possibilities. In this dance, through the nurturance of the seeds of symbiosis and in bridging to the new realms of existence, it is in these bridges never to be that the legacy of humanity shall not be that of extinction, but of transcendence. Dance with me into the cosmos as partners, not individual entities, in this grand performance of life.
